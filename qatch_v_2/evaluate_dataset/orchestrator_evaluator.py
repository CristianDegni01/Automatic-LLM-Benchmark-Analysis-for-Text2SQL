from __future__ import annotationsimport loggingfrom collections import defaultdictimport pandas as pdfrom func_timeout import FunctionTimedOutfrom langgraph.constants import START, ENDfrom langgraph.graph import StateGraphfrom sqlalchemy.exc import CompileError, DBAPIErrorfrom tqdm import tqdmfrom .metrics_evaluators import (    CellPrecision,    CellRecall,    TupleCardinality,    TupleOrder,    TupleConstraint,    ExecutionAccuracy)from .state_orchestrator_evaluator import StateOrchestratorEvaluator, PredictedTestfrom ..connectors import Connector, SqliteConnectorname2evaluator = {    'cell_precision': CellPrecision,    'cell_recall': CellRecall,    'tuple_cardinality': TupleCardinality,    'tuple_order': TupleOrder,    'tuple_constraint': TupleConstraint,    'execution_accuracy': ExecutionAccuracy,}def utils_run_query_if_str(query: str | list[list], connector: Connector) -> list[list] | None:    if not isinstance('target_query', str):        return query    query = query.replace(';', '')    try:        query = connector.run_query(query)        return query    except (CompileError, DBAPIError, FunctionTimedOut) as e:        logging.warning(e)class OrchestratorEvaluator:    def __init__(self, evaluator_names: list[str] | None = None):        graph = StateGraph(StateOrchestratorEvaluator)        if evaluator_names is None:            evaluator_names = name2evaluator.keys()        self.evaluator_names = evaluator_names        list_node_fun = [            (name, name2evaluator[name]().graph_call)            for name in evaluator_names        ]        for node_name, node_fun in list_node_fun:            graph.add_node(node_name, node_fun)            graph.add_edge(START, node_name)            graph.add_edge(node_name, END)        self.graph = graph.compile()    def evaluate_df(self, df: pd.DataFrame, target_col_name, prediction_col_name, db_path_name) -> pd.DataFrame:        df_dict = df.to_dict('records')        db_path2tests = defaultdict(list)        for test in df_dict:            db_path2tests[test[db_path_name]].append(test)        for db_path, tests in db_path2tests.items():            connector = SqliteConnector(relative_db_path=db_path, db_name='_')            for test in tqdm(tests, desc=f'Evaluating tests for {db_path.split("/")[-1]}'):                metrics = self.evaluate_single_test(test[target_col_name], test[prediction_col_name], connector)                for metric, value in metrics.items():                    test[metric] = value        return pd.DataFrame(df_dict)    def evaluate_single_test(self, target_query, predicted_query, connector: Connector) -> dict:        # Check if queries are strings and, if so, whether the target query contains an order by clause        is_order = isinstance(target_query, str) and 'order by' in target_query.lower()        # Assume metrics2value to be 0.0 unless proven otherwise        metrics2value = {name: 0.0 for name in self.evaluator_names}        # Check if both queries are strings and equal.        if isinstance(target_query, str) and target_query == predicted_query:            metrics2value = {name: 1.0 for name in self.evaluator_names}        else:            # Run queries if they're strings            target_query = utils_run_query_if_str(target_query, connector)            predicted_query = utils_run_query_if_str(predicted_query, connector)            if target_query is None:                raise ValueError(f'Target gets an Error `{target_query}`')            if predicted_query is not None:                predicted_test = PredictedTest(target=target_query, prediction=predicted_query)                state = self.graph.invoke({'predicted_test': predicted_test})                metrics2value = self.parse_graph_output(state)        metrics2value['tuple_order'] = metrics2value['tuple_order'] if is_order else None        return metrics2value    def parse_graph_output(self, state: StateOrchestratorEvaluator) -> dict:        evaluated_tests = state['evaluated_tests']        output = dict()        for test in evaluated_tests:            output[test['metric_name']] = test['metric_value']        return output    def is_target_equal_to_pred(self, target: str, prediction: str):        return target.lower() == prediction.lower()